{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0693ab",
   "metadata": {},
   "source": [
    "# CENTRALESUPELEC 2021-2022 - Intelligence Artificielle pour le Véhicule Autonome\n",
    "\n",
    "Entrainement d'un réseau de neurones de classification pour le passage des feux tricolores.\n",
    "\n",
    "L'objet de l'étude est d'entrainer un réseau qui à partir de l'état dynamique du véhicule et de l'observation du feu tricolore devant lui va décider dans quel mode ce véhicule doit se trouver. Le concepteur a défini 5 modes. Il s'agit donc de définir à chaque pas de temps quel est le bon mode. C'est un problème de classification.\n",
    "\n",
    "L'entrainement retenu est un entrainement supervisé. Cet entrainement va se dérouler en 4 étapes:\n",
    "\n",
    "1) La phase de récolte des données d'entrainement, de tests et de validation. Ces données seront constituées dee entrées du réseau de neurones et de la vérité terrain (ground truth) associée à chaque vecteur d'entrée.\n",
    "\n",
    "2) La phase de préparation des datas pour les mettre dans un format adapté à l'uotil d'entrainement de keras\n",
    "\n",
    "3) La phase d'entrainement proprement dite\n",
    "\n",
    "4) La pahse de validation \n",
    "\n",
    "Si la phase de validation donne des résultats satisfaisants, le réseau sera alors bon pour son implantation opérationnelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5530e1",
   "metadata": {},
   "source": [
    "# 1) Récolte des données d'entrainement\n",
    "\n",
    "Dans notre étude, on sait le choix de produire ces données par simulation. La simulation va simuler une voiture arrivant à une intersection protégée par un feu et son comportement sera défini par de algorithmes classiques\" (cf. la présentation). On enregistre à chaque pas de temps les paramètres souhaités.\n",
    "\n",
    "Sans simulation, il aurait fallu prendre une voiture, l'instrumenter pour être capable d'enregistrer les différents paramètres et réaliser un certain nombre de roulages réels, sur la voie publique, pour accumuler suffisament de données.\n",
    "\n",
    "On a réalisé 3000 simulations monte-carlo (sont tirées au sort en début de simulation: la vitesse de la voiture au T0 de la simulaton, la couleur du feu à ce T0, le temps écoulé depuis que le feu est passé à cette couleur et enfin la distance à partir de laquelle la voiture verra la couleur du feu. Pour chacune de ces grandeurs un tirage aléatoire de distribution uniforme est utilisé.\n",
    "\n",
    "La vitesse initiale est tirée entre 6.1 et 13m/s\n",
    "La distance de détection du feu est tirée entre 10 et 40m.\n",
    "\n",
    "Générer 3000 scenarios aura pris 248 secondes. Combien de temps un roulage réel aurait-il demandé pour enregistrer la même quantité de données?\n",
    "(Rearque: on se limite à 3000 cas pour limiter la taille de la base à manipuler en direct durant ce cours!!! 4,4Go non compressée)\n",
    "\n",
    "Le résultat est un fichier au format csv ou chaque ligne correspond à un pas de temps. Passage d'un cas à l'autre séquentiellement, sans mesure particulière au niveau de la transition. \n",
    "\n",
    "Variables stockées sur une ligne:\n",
    "\tla position de Ego car (en m)\n",
    "\tla vitesse d'ego car (en m/s) \n",
    "\tl'accélératio  d'ego car (en m/s²)\n",
    "\tHot1(n-1)\n",
    "\tHot2(n-1)\n",
    "\tHot3(n-1)\n",
    "(Hot1(n-1), Hot2(n-1), Hot3(n-1)) one hot encoding de la couleur du feu au pas de temps précédent: (0,0,0): feu non détecté; (1;0;0): feu vert; (0;1;0): feu orange; (0;0;1): feu rouge\n",
    "\n",
    "\tHot1(n)\n",
    "\tHot2(n)\n",
    "\tHot3(n)\n",
    "(Hot1(n), Hot2(n), Hot3(n)) = one hot encoding de la couleur du feu au pas de temps courant:(0,0,0): feu non détecté; (1;0;0): feu vert; (0;1;0): feu orange; (0;0;1): feu rouge\n",
    "\n",
    "\tHotMod0(n-1)\n",
    "\tHotMod1(n-1)\n",
    "\tHotMod2(n-1)\n",
    "\tHotMod3(n-1)\n",
    "\tHotMod4(n-1)\n",
    "(HotMod0(n-1); HotMod1(n-1); HotMod2(n-1); HotMod3(n-1); HotMod4(n-1)) = one hot encoding de la prediction du Mode faite au pas de temps précédent. ModePredit(n-1) = np.argmax((HotMod0(n-1); HotMod1(n-1); HotMod2(n-1); HotMod3(n-1); HotMod4(n-1))\n",
    "\n",
    "\tHotMod0GT(n)\n",
    "\tHotMod1GT(n)\n",
    "\tHotMod2GT(n)\n",
    "\tHotMod3GT(n)\n",
    "\tHotMod4GT(n)\n",
    " (HotMod0GT(n); HotMod1GT(n); HotMod2GT(n); HotMod3GT(n); HotMod4GT(n)) = one hot encoding de la vérité terrain pour l'enregistrement courant. Valeur \"vérité terrain\" du Mode au pas de temps présent = np.argmax((HotMod0GT(n); HotMod1GT(n); HotMod2GT(n); HotMod3GT(n); HotMod4GT(n))\n",
    "\n",
    "Les 14 premières variables sont celles en entrée du réseau pour apprentissage; les 5 dernières = ground truth associée.\n",
    "\n",
    "Le fichier se nomme: CS2021_CLASSIFIER_BDD_3000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4658f",
   "metadata": {},
   "source": [
    "# 2) Préparation des datas pour l'entrainement\n",
    "\n",
    "Il faut transférer ces données du fichier txt vers un tableau (array numpy). On passe d'abord par une liste puis on trasfère la liste dans un numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49f218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Start loading Dataset\n",
      "Loading duration= 0.07939887046813965 s\n",
      "Start loading Dataset\n",
      "Loading duration= 0.867145299911499 s\n"
     ]
    }
   ],
   "source": [
    "#importations\n",
    "import numpy as np\n",
    "import time\n",
    "from utils import load_data\n",
    "\n",
    "# define dataset\n",
    "DataSize='3'\n",
    "DataName='CS2021_CLASSIFIER_BDD_'+DataSize\n",
    "path = \"data/\"+DataName+\".txt\"\n",
    "\n",
    "TestDataSize='25'\n",
    "TestDataName='CS2021_CLASSIFIER_TEST_'+TestDataSize\n",
    "test_path = \"data/\"+TestDataName+\".txt\"\n",
    "\n",
    "input_size = 14\n",
    "output_size = 5\n",
    "\n",
    "InputNN, GroundTruth, LengthData = load_data(path, input_size, output_size)\n",
    "TestInputNN, TestGroundTruth, TestLenghtData = load_data(test_path, input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf8fe1",
   "metadata": {},
   "source": [
    "# 3) Phase d'entrainement - construction du modèle\n",
    "\n",
    "Avant d'entrainer il faut construire en premier lieu le réseau. Le concepteur décide de prendre un réseau:\n",
    "\n",
    "MLP , avec une première couche de 64 neurones avec fonction d'activation \"relu\", une deuxième couche de 32 neurones avec une fonction d'activation \"relu\", et une couche de sortie de 5 neurones (la taille du one hot encoding du mode rédit) avec fonction d'activation \"softmax\" (car classification).\n",
    "\n",
    "Pour l'entrainement on prendre une loss de type \"categorical_crossentropy\" adaptée aux problèmes de classification avec un schéma d'optimisation adam \"de base\".\n",
    "\n",
    "Ce modèle est construit et compilé sous keras, après les imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f398009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 3,205\n",
      "Trainable params: 3,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#importations\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Model building\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=input_size, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "#Model compilation\n",
    "opt_adam = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adam , metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a821c",
   "metadata": {},
   "source": [
    "# 3-2) Phase d'entrainement - Données d'entrainement vs données de test\n",
    "\n",
    "\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228d9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database length = 6764\n",
      "Test database length = 75019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75019"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training base and test base definition according to the ratio we defined\n",
    "Ratio = 0.8\n",
    "TrainingSize = int(LengthData*Ratio)\n",
    "TestSize = LengthData - TrainingSize\n",
    "\n",
    "# Train base extraction\n",
    "#X_DNN_Train = InputNN[0:(TrainingSize-1),:]\n",
    "#Y_DNN_Train = GroundTruth[0:(TrainingSize-1) , :] \n",
    "\n",
    "X_DNN_Train = InputNN\n",
    "Y_DNN_Train = GroundTruth\n",
    "\n",
    "# Test base extraction\n",
    "#X_DNN_Test = InputNN[TrainingSize : LengthData ,:]\n",
    "#Y_DNN_Test = GroundTruth[TrainingSize : LengthData , :]\n",
    "\n",
    "X_DNN_Test = TestInputNN\n",
    "Y_DNN_Test = TestGroundTruth\n",
    "\n",
    "print(f\"Database length = {LengthData}\")\n",
    "print(f\"Test database length = {TestLenghtData}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060b95a",
   "metadata": {},
   "source": [
    "# 3-3) Phase entrainement - entrainement\n",
    "\n",
    "\n",
    "bla bla bla epoch, batch size...history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522ae680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68/68 - 1s - loss: 0.2580 - accuracy: 0.9993 - val_loss: 0.3720 - val_accuracy: 0.9739\n",
      "Epoch 2/100\n",
      "68/68 - 1s - loss: 0.2165 - accuracy: 0.9993 - val_loss: 0.3327 - val_accuracy: 0.9748\n",
      "Epoch 3/100\n",
      "68/68 - 0s - loss: 0.1803 - accuracy: 0.9993 - val_loss: 0.2999 - val_accuracy: 0.9753\n",
      "Epoch 4/100\n",
      "68/68 - 0s - loss: 0.1500 - accuracy: 0.9993 - val_loss: 0.2749 - val_accuracy: 0.9762\n",
      "Epoch 5/100\n",
      "68/68 - 0s - loss: 0.1267 - accuracy: 0.9993 - val_loss: 0.2553 - val_accuracy: 0.9769\n",
      "Epoch 6/100\n",
      "68/68 - 0s - loss: 0.1071 - accuracy: 0.9993 - val_loss: 0.2405 - val_accuracy: 0.9771\n",
      "Epoch 7/100\n",
      "68/68 - 0s - loss: 0.0917 - accuracy: 0.9993 - val_loss: 0.2281 - val_accuracy: 0.9776\n",
      "Epoch 8/100\n",
      "68/68 - 0s - loss: 0.0791 - accuracy: 0.9993 - val_loss: 0.2184 - val_accuracy: 0.9780\n",
      "Epoch 9/100\n",
      "68/68 - 0s - loss: 0.0686 - accuracy: 0.9993 - val_loss: 0.2111 - val_accuracy: 0.9781\n",
      "Epoch 10/100\n",
      "68/68 - 0s - loss: 0.0597 - accuracy: 0.9993 - val_loss: 0.2045 - val_accuracy: 0.9786\n",
      "Epoch 11/100\n",
      "68/68 - 1s - loss: 0.0524 - accuracy: 0.9993 - val_loss: 0.1994 - val_accuracy: 0.9788\n",
      "Epoch 12/100\n",
      "68/68 - 0s - loss: 0.0462 - accuracy: 0.9993 - val_loss: 0.1954 - val_accuracy: 0.9791\n",
      "Epoch 13/100\n",
      "68/68 - 0s - loss: 0.0406 - accuracy: 0.9993 - val_loss: 0.1921 - val_accuracy: 0.9791\n",
      "Epoch 14/100\n",
      "68/68 - 0s - loss: 0.0358 - accuracy: 0.9993 - val_loss: 0.1888 - val_accuracy: 0.9796\n",
      "Epoch 15/100\n",
      "68/68 - 0s - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.1870 - val_accuracy: 0.9797\n",
      "Epoch 16/100\n",
      "68/68 - 0s - loss: 0.0286 - accuracy: 0.9993 - val_loss: 0.1849 - val_accuracy: 0.9800\n",
      "Epoch 17/100\n",
      "68/68 - 0s - loss: 0.0258 - accuracy: 0.9993 - val_loss: 0.1830 - val_accuracy: 0.9805\n",
      "Epoch 18/100\n",
      "68/68 - 0s - loss: 0.0233 - accuracy: 0.9993 - val_loss: 0.1821 - val_accuracy: 0.9805\n",
      "Epoch 19/100\n",
      "68/68 - 1s - loss: 0.0213 - accuracy: 0.9993 - val_loss: 0.1810 - val_accuracy: 0.9808\n",
      "Epoch 20/100\n",
      "68/68 - 0s - loss: 0.0194 - accuracy: 0.9993 - val_loss: 0.1804 - val_accuracy: 0.9806\n",
      "Epoch 21/100\n",
      "68/68 - 1s - loss: 0.0179 - accuracy: 0.9993 - val_loss: 0.1799 - val_accuracy: 0.9809\n",
      "Epoch 22/100\n",
      "68/68 - 1s - loss: 0.0165 - accuracy: 0.9993 - val_loss: 0.1798 - val_accuracy: 0.9809\n",
      "Epoch 23/100\n",
      "68/68 - 0s - loss: 0.0152 - accuracy: 0.9993 - val_loss: 0.1798 - val_accuracy: 0.9811\n",
      "Epoch 24/100\n",
      "68/68 - 0s - loss: 0.0142 - accuracy: 0.9993 - val_loss: 0.1789 - val_accuracy: 0.9813\n",
      "Epoch 25/100\n",
      "68/68 - 0s - loss: 0.0132 - accuracy: 0.9993 - val_loss: 0.1788 - val_accuracy: 0.9813\n",
      "Epoch 26/100\n",
      "68/68 - 0s - loss: 0.0123 - accuracy: 0.9993 - val_loss: 0.1793 - val_accuracy: 0.9812\n",
      "Epoch 27/100\n",
      "68/68 - 1s - loss: 0.0115 - accuracy: 0.9993 - val_loss: 0.1789 - val_accuracy: 0.9816\n",
      "Epoch 28/100\n",
      "68/68 - 0s - loss: 0.0108 - accuracy: 0.9993 - val_loss: 0.1791 - val_accuracy: 0.9816\n",
      "Epoch 29/100\n",
      "68/68 - 0s - loss: 0.0102 - accuracy: 0.9993 - val_loss: 0.1790 - val_accuracy: 0.9818\n",
      "Epoch 30/100\n",
      "68/68 - 0s - loss: 0.0096 - accuracy: 0.9993 - val_loss: 0.1799 - val_accuracy: 0.9818\n",
      "Epoch 31/100\n",
      "68/68 - 0s - loss: 0.0091 - accuracy: 0.9993 - val_loss: 0.1793 - val_accuracy: 0.9819\n",
      "Epoch 32/100\n",
      "68/68 - 0s - loss: 0.0086 - accuracy: 0.9993 - val_loss: 0.1792 - val_accuracy: 0.9819\n",
      "Epoch 33/100\n",
      "68/68 - 0s - loss: 0.0082 - accuracy: 0.9993 - val_loss: 0.1803 - val_accuracy: 0.9818\n",
      "Epoch 34/100\n",
      "68/68 - 0s - loss: 0.0078 - accuracy: 0.9993 - val_loss: 0.1803 - val_accuracy: 0.9819\n",
      "Epoch 35/100\n",
      "68/68 - 0s - loss: 0.0074 - accuracy: 0.9993 - val_loss: 0.1806 - val_accuracy: 0.9820\n",
      "Epoch 36/100\n",
      "68/68 - 0s - loss: 0.0071 - accuracy: 0.9993 - val_loss: 0.1809 - val_accuracy: 0.9821\n",
      "Epoch 37/100\n",
      "68/68 - 1s - loss: 0.0068 - accuracy: 0.9993 - val_loss: 0.1812 - val_accuracy: 0.9821\n",
      "Epoch 38/100\n",
      "68/68 - 0s - loss: 0.0065 - accuracy: 0.9993 - val_loss: 0.1812 - val_accuracy: 0.9821\n",
      "Epoch 39/100\n",
      "68/68 - 1s - loss: 0.0062 - accuracy: 0.9993 - val_loss: 0.1813 - val_accuracy: 0.9822\n",
      "Epoch 40/100\n",
      "68/68 - 0s - loss: 0.0060 - accuracy: 0.9993 - val_loss: 0.1819 - val_accuracy: 0.9822\n",
      "Epoch 41/100\n",
      "68/68 - 0s - loss: 0.0057 - accuracy: 0.9993 - val_loss: 0.1823 - val_accuracy: 0.9822\n",
      "Epoch 42/100\n",
      "68/68 - 0s - loss: 0.0055 - accuracy: 0.9993 - val_loss: 0.1824 - val_accuracy: 0.9822\n",
      "Epoch 43/100\n",
      "68/68 - 0s - loss: 0.0054 - accuracy: 0.9993 - val_loss: 0.1833 - val_accuracy: 0.9823\n",
      "Epoch 44/100\n",
      "68/68 - 0s - loss: 0.0051 - accuracy: 0.9993 - val_loss: 0.1838 - val_accuracy: 0.9823\n",
      "Epoch 45/100\n",
      "68/68 - 0s - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.1834 - val_accuracy: 0.9823\n",
      "Epoch 46/100\n",
      "68/68 - 0s - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.1838 - val_accuracy: 0.9824\n",
      "Epoch 47/100\n",
      "68/68 - 0s - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.1847 - val_accuracy: 0.9822\n",
      "Epoch 48/100\n",
      "68/68 - 1s - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.1851 - val_accuracy: 0.9823\n",
      "Epoch 49/100\n",
      "68/68 - 1s - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.1851 - val_accuracy: 0.9825\n",
      "Epoch 50/100\n",
      "68/68 - 0s - loss: 0.0043 - accuracy: 0.9993 - val_loss: 0.1854 - val_accuracy: 0.9824\n",
      "Epoch 51/100\n",
      "68/68 - 0s - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.1860 - val_accuracy: 0.9825\n",
      "Epoch 52/100\n",
      "68/68 - 0s - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.1864 - val_accuracy: 0.9824\n",
      "Epoch 53/100\n",
      "68/68 - 0s - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.1865 - val_accuracy: 0.9825\n",
      "Epoch 54/100\n",
      "68/68 - 1s - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.1869 - val_accuracy: 0.9825\n",
      "Epoch 55/100\n",
      "68/68 - 1s - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.1873 - val_accuracy: 0.9825\n",
      "Epoch 56/100\n",
      "68/68 - 1s - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.1876 - val_accuracy: 0.9826\n",
      "Epoch 57/100\n",
      "68/68 - 1s - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.1884 - val_accuracy: 0.9825\n",
      "Epoch 58/100\n",
      "68/68 - 1s - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.1882 - val_accuracy: 0.9826\n",
      "Epoch 59/100\n",
      "68/68 - 1s - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.1888 - val_accuracy: 0.9826\n",
      "Epoch 60/100\n",
      "68/68 - 0s - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.1893 - val_accuracy: 0.9826\n",
      "Epoch 61/100\n",
      "68/68 - 0s - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.1891 - val_accuracy: 0.9827\n",
      "Epoch 62/100\n",
      "68/68 - 0s - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.1899 - val_accuracy: 0.9827\n",
      "Epoch 63/100\n",
      "68/68 - 0s - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.1900 - val_accuracy: 0.9828\n",
      "Epoch 64/100\n",
      "68/68 - 0s - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.1905 - val_accuracy: 0.9827\n",
      "Epoch 65/100\n",
      "68/68 - 0s - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.1909 - val_accuracy: 0.9827\n",
      "Epoch 66/100\n",
      "68/68 - 0s - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.1913 - val_accuracy: 0.9827\n",
      "Epoch 67/100\n",
      "68/68 - 1s - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.1918 - val_accuracy: 0.9827\n",
      "Epoch 68/100\n",
      "68/68 - 0s - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.1923 - val_accuracy: 0.9828\n",
      "Epoch 69/100\n",
      "68/68 - 0s - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.1922 - val_accuracy: 0.9828\n",
      "Epoch 70/100\n",
      "68/68 - 0s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.1922 - val_accuracy: 0.9829\n",
      "Epoch 71/100\n",
      "68/68 - 0s - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.1930 - val_accuracy: 0.9829\n",
      "Epoch 72/100\n",
      "68/68 - 0s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1928 - val_accuracy: 0.9829\n",
      "Epoch 73/100\n",
      "68/68 - 0s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1936 - val_accuracy: 0.9829\n",
      "Epoch 74/100\n",
      "68/68 - 0s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1940 - val_accuracy: 0.9830\n",
      "Epoch 75/100\n",
      "68/68 - 0s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1943 - val_accuracy: 0.9829\n",
      "Epoch 76/100\n",
      "68/68 - 0s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1942 - val_accuracy: 0.9830\n",
      "Epoch 77/100\n",
      "68/68 - 0s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.1946 - val_accuracy: 0.9830\n",
      "Epoch 78/100\n",
      "68/68 - 0s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.1951 - val_accuracy: 0.9830\n",
      "Epoch 79/100\n",
      "68/68 - 0s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.1949 - val_accuracy: 0.9830\n",
      "Epoch 80/100\n",
      "68/68 - 0s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.1952 - val_accuracy: 0.9830\n",
      "Epoch 81/100\n",
      "68/68 - 0s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.1959 - val_accuracy: 0.9830\n",
      "Epoch 82/100\n",
      "68/68 - 1s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.1964 - val_accuracy: 0.9830\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 - 1s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.1963 - val_accuracy: 0.9831\n",
      "Epoch 84/100\n",
      "68/68 - 1s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.1965 - val_accuracy: 0.9831\n",
      "Epoch 85/100\n",
      "68/68 - 1s - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.1978 - val_accuracy: 0.9831\n",
      "Epoch 86/100\n",
      "68/68 - 1s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.1977 - val_accuracy: 0.9831\n",
      "Epoch 87/100\n",
      "68/68 - 0s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.1976 - val_accuracy: 0.9831\n",
      "Epoch 88/100\n",
      "68/68 - 0s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.1983 - val_accuracy: 0.9831\n",
      "Epoch 89/100\n",
      "68/68 - 0s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.1991 - val_accuracy: 0.9831\n",
      "Epoch 90/100\n",
      "68/68 - 0s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.1987 - val_accuracy: 0.9832\n",
      "Epoch 91/100\n",
      "68/68 - 0s - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.1990 - val_accuracy: 0.9831\n",
      "Epoch 92/100\n",
      "68/68 - 0s - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.1995 - val_accuracy: 0.9832\n",
      "Epoch 93/100\n",
      "68/68 - 0s - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.1997 - val_accuracy: 0.9832\n",
      "Epoch 94/100\n",
      "68/68 - 0s - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2001 - val_accuracy: 0.9832\n",
      "Epoch 95/100\n",
      "68/68 - 0s - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2005 - val_accuracy: 0.9832\n",
      "Epoch 96/100\n",
      "68/68 - 0s - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2010 - val_accuracy: 0.9832\n",
      "Epoch 97/100\n",
      "68/68 - 1s - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2009 - val_accuracy: 0.9832\n",
      "Epoch 98/100\n",
      "68/68 - 0s - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.2014 - val_accuracy: 0.9832\n",
      "Epoch 99/100\n",
      "68/68 - 0s - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.2013 - val_accuracy: 0.9833\n",
      "Epoch 100/100\n",
      "68/68 - 0s - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.2019 - val_accuracy: 0.9832\n",
      "\n",
      ">>>>>>>>>>> Training completed in 50.27s\n",
      "Model saved at ---> models/CS2021_CLASSIFIER_BDD_3_epoch100_batchsize100_20211007-135745.h5\n"
     ]
    }
   ],
   "source": [
    "#classes\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "#training process\n",
    "nb_epochs= 100\n",
    "batch_size =100\n",
    "\n",
    "time_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_dir = f\"logs/fit/{DataName}_epoch{nb_epochs}_batchsize{batch_size}_{time_date}\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "GoTrain=time.time()\n",
    "model.fit(X_DNN_Train, Y_DNN_Train, epochs=nb_epochs, validation_data = (X_DNN_Test, Y_DNN_Test), batch_size=batch_size, verbose = 2, callbacks=[tensorboard_callback])\n",
    "\n",
    "print(f\"\\n>>>>>>>>>>> Training completed in {(time.time() -GoTrain):.2f}s\")\n",
    "\n",
    "#Model saving\n",
    "ModelName = f\"{DataName}_epoch{nb_epochs}_batchsize{batch_size}_{time_date}.h5\"\n",
    "model.save(\"models/\"+ModelName)\n",
    "print(f\"Model saved at ---> models/{ModelName}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612aa93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11968), started 0:01:42 ago. (Use '!kill 11968' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-19bc5ed3949b71d3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-19bc5ed3949b71d3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"logs/fit\" --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa0e7e",
   "metadata": {},
   "source": [
    "# 4) Validation\n",
    "\n",
    "bla bla bla, on essaie sur la base de train et sur une autre base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e7a765",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22004/1038513827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate the model - first on the training base\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mYTrainPredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_DNN_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mNbErrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# evaluate the model - first on the training base\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "YTrainPredict = model.predict(X_DNN_Train)\n",
    "NbErrors = 0\n",
    "for index in range(TrainingSize-1):\n",
    "    TrainIMode[index] = np.argmax(Y_DNN_Train [index,:])\n",
    "    TrainIMode[index] = np.argmax(YTrainPredict [index,:])\n",
    "    if abs(TrainIMode[index] - TrainIMode[index]) > 0.1:\n",
    "        NbErrors +=1\n",
    "\n",
    "#display training results\n",
    "plt.subplot(211)\n",
    "plt.title('IMode prediction error on the training base (GT ---)')\n",
    "plt.plot(TrainPredictedIMode[:])\n",
    "plt.plot(TrainIMode[:],'--')\n",
    "plt.subplot(212)\n",
    "plt.title('Prediction error ')\n",
    "plt.plot(TrainPredictedIMode[:] -TrainIMode[:])\n",
    "plt.show()\n",
    "\n",
    "print('>>>>>>>>>>>>>> Number of Prediction Errors on the training base= ', NbErrors, ' out of', (TrainingSize-1),\n",
    "      ' samples --> success ratio= ', (1-(NbErrors/(TrainingSize-1)))*100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3a3f7",
   "metadata": {},
   "source": [
    "bla bla bla test base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "### second, on the test base\n",
    "YTestPredict= model.predict(X_DNN_Test)\n",
    "NbErrors = 0\n",
    "for index in range(TestSize):\n",
    "    TestIMode[index] = np.argmax(Y_DNN_Test [index,:])\n",
    "    TestPredictedIMode[index] = np.argmax(YTestPredict [index,:])\n",
    "    if abs(TestIMode[index]-TestPredictedIMode[index]) >0.1:\n",
    "        NbErrors +=1\n",
    "        \n",
    "#display results\n",
    "plt.subplot(211)\n",
    "plt.title('IMode prediction error on the test base (GT ---)')\n",
    "plt.plot(TestPredictedIMode[:])\n",
    "plt.plot(TestIMode[:],'--')\n",
    "plt.subplot(212)\n",
    "plt.title('Prediction error ')\n",
    "plt.plot(TestPredictedIMode[:] -TestIMode[:])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\">>>>>>>>>>>>>> Number of Prediction Errors on the training base = {NbErrors} out of {(TestSize)} samples\")\n",
    "print(f\">>>>>>>>>>>>>> Success ratio = {(1-(NbErrors/TestSize))*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fac1a",
   "metadata": {},
   "source": [
    "bla bla bla sur une autre base de données générées selon le même processus que la base d'entrainement mais avec des tirages initiaux différents (random seed = 1999 ald 1993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2339cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "DataSize2='25'\n",
    "DataName2='CS2021_CLASSIFIER_BDD_'+DataSize2\n",
    "path2 = \"data/\"+DataName2+\".txt\"\n",
    "\n",
    "ValidInputNN, ValidGroundTruth, ValidDataLength = load_data(path2, input_size, output_size)\n",
    "\n",
    "#Model prediction\n",
    "\n",
    "YValidPredict = model.predict(X_DNN_Valid)\n",
    "NbError = 0\n",
    "for index in range(ValidDataLength-1):\n",
    "    ValidIMode[index] = np.argmax(Y_DNN_Valid [index,:])\n",
    "    ValidPredictedIMode[index] = np.argmax(YValidPredict [index,:])\n",
    "    ValidPredError[index] = ValidIMode[index] - ValidPredictedIMode[index]\n",
    "    if (abs(ValidPredError[index]) > 0.1):\n",
    "        NbError +=1\n",
    "        \n",
    "\n",
    "    \n",
    "#display results\n",
    "plt.subplot(211)\n",
    "plt.title('IMode prediction error on the Validation base (GT ---)')\n",
    "plt.plot(ValidPredictedIMode[:])\n",
    "plt.plot(ValidIMode[:],'--')\n",
    "plt.subplot(212)\n",
    "plt.title('Prediction error ')\n",
    "plt.plot(ValidPredError[:])\n",
    "plt.show()\n",
    "\n",
    "print('>>>>>>>>>>>>>> Number of Prediction Errors = ', NbError, ' out of', ValidDataLength,\n",
    "      ' samples --> success ratio= ', (1-(NbError/ValidDataLength))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7413ab",
   "metadata": {},
   "source": [
    "# 5) Validation dans une simulation boucle fermée\n",
    "\n",
    "blabla... on peut se tromper mais vu la dynamique des décisions vis à vis de celle de la voiture une erreur ponctuelle de prédiction peut ne pas avoir d'effet perceptible. au pire Deltax = Deltat*v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4182019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Sim_TFlight\n",
    "\n",
    "sim = Sim_TFlight(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e628ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_rendering_runs = 5\n",
    "\n",
    "for i in range(N_rendering_runs):\n",
    "    render = True\n",
    "    while render:\n",
    "        sim.step()\n",
    "        render = sim.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c216c",
   "metadata": {},
   "source": [
    "# 6) Discussion sur les hyperparamètres et la taille de la base de données\n",
    "\n",
    "Au vu des résultats de la simulation en boucle fermée.\n",
    "\n",
    "On remplit en direct un tableau Résultats simu boucle fermée = f( epochs, batch, database size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add584b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
